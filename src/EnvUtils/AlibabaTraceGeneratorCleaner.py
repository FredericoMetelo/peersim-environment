"""
Clean up the alibaba trace as generated by the All-less/trace-generator.
src of the tool: https://github.com/All-less/trace-generator
"""
import json
import os
import networkx as nx

def read_data_form_csv(file_path: str):
    """
    Read the data from the csv file and return a list of the data
    :param file_path:
    :return:
    """
    jobs_dictionary = {}
    with open(file_path, "r") as file:
        independent_task_id = 0
        for line in file:
            data = line.split(",")
            job_arrival = data[0]
            job_id = data[1]
            task_id_dependencies = data[2].split("_")
            if "task" in task_id_dependencies[0]:
                task_id = f"{independent_task_id}"
                task_dependencies = []
                independent_task_id += 1
            else:
                task_id = task_id_dependencies[0][1:] # Ignore the first letter.
                task_dependencies = task_id_dependencies[1:]
            task_duration = float(data[3])
            task_cpu = float(data[4])
            task_mem = float(data[5])
            task_no_instances = float(data[6])
            job_dict = jobs_dictionary.get(job_id, None)
            if job_dict is None:
                jobs_dictionary[job_id] = {}
            jobs_dictionary[job_id][task_id] = [task_id, task_dependencies, task_duration, task_cpu, task_mem, task_no_instances]

    return jobs_dictionary

def get_longest_path(job: dict):
    """
    Sort the tasks in a job in topological order, given that Job is represented as a dictionary with entries of the form
     task: [dependencies].
    Generates the NetworkX graph and then uses the method dag_longest_path() to get the longest path in the graph.
    This method uses the code at:
        https://networkx.org/documentation/stable/_modules/networkx/algorithms/dag.html#dag_longest_path

    :param job:
    :return: the critial path of the job
    """
    # Create a directed graph
    G = nx.DiGraph()

    # Add nodes (tasks) to the graph
    for task in job:
        if task not in G.nodes:
            G.add_node(task)
        dependencies = job[task][1]
        for dependency in dependencies:
            task_duration = job[dependency][2]
            if dependency not in G.nodes:
                G.add_node(dependency)
            G.add_edge(dependency, task, weight=task_duration)
    critical_path = nx.dag_longest_path(G)
    return critical_path


def process_data(filename: str = "alibaba_trace.csv"):
    """
    Process the data by reading the original csv file and extracting the required information for each job, then compiling
    it into the format accepted by PeerSim. And lastly writing the output to a new file. Which can be passed in the configs
    
    We need to compile 3 attributed for each job:

    
    :return: 
    """
    data = read_data_form_csv(filename)
    output_data = {}
    simple_entry = {}
    for job in data:
        job_dict = data[job]
        critical_path = get_longest_path(job_dict)
        total_cpu_resources = 0
        total_mem_resources = 0
        total_instances = 0
        total_duration = 0

        max_cpu = float("-inf")
        max_mem = float("-inf")

        cp_cpu_resources = 0
        cp_mem_resources = 0
        cp_duration = 0

        for task in job_dict:
            total_cpu_resources += job_dict[task][3]
            total_mem_resources += job_dict[task][4]
            total_instances += job_dict[task][5]
            total_duration += job_dict[task][2]
            if job_dict[task][3] > max_cpu:
                max_cpu = job_dict[task][3]
            if job_dict[task][4] > max_mem:
                max_mem = job_dict[task][4]
        for task in critical_path:
            cp_cpu_resources += job_dict[task][3]
            cp_mem_resources += job_dict[task][4]
            cp_duration += job_dict[task][2]

        # Write the job to the new file
        output_data[job] = {}
        output_data[job]["critical_path"] = critical_path
        output_data[job]["tasks"] = job_dict

        output_data[job]["total_resources_cpu"] = total_cpu_resources
        output_data[job]["total_resources_mem"] = total_mem_resources
        output_data[job]["total_resources_instances"] = total_instances
        output_data[job]["total_duration"] = total_duration

        output_data[job]["critical_path_resources_cpu"] = cp_cpu_resources
        output_data[job]["critical_path_resources_mem"] = cp_mem_resources
        output_data[job]["critical_path_duration"] = cp_duration

        simple_entry[job] = {}
        simple_entry[job]["critical_path_resources_cpu"] = cp_cpu_resources
        simple_entry[job]["critical_path_resources_mem"] = cp_mem_resources
        simple_entry[job]["total_resources_cpu"] = total_cpu_resources
        simple_entry[job]["total_resources_mem"] = total_mem_resources
        simple_entry[job]["total_resources_instances"] = total_instances
        simple_entry[job]["total_resources_duration"] = total_duration
        simple_entry[job]["critical_path_duration"] = cp_duration
        simple_entry[job]["max_mem"] = max_mem
        simple_entry[job]["max_cpu"] = max_cpu
    return output_data, simple_entry


if __name__ == "__main__":
    print("Cleaning up the alibaba trace generated by the All-less/trace-generator")
    output_data, simple_data = process_data("./Datasets/batch_task.csv")
    with open("./Datasets/alibaba_trace_cleaned.json", "w") as file:
        json.dump(simple_data, file)
    print("Done.")

