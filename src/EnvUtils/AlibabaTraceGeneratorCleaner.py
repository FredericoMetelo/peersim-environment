"""
Clean up the alibaba trace as generated by the All-less/trace-generator.
src of the tool: https://github.com/All-less/trace-generator
"""
import json
import os
import networkx as nx

def read_data_form_csv(file_path: str):
    """
    Read the data from the csv file and return a list of the data
    :param file_path:
    :return:
    """
    jobs_dictionary = {}
    with open(file_path, "r") as file:
        independent_task_id = 0
        for line in file:
            data = line.split(",")
            job_arrival = data[0]
            job_id = data[1]
            task_id_dependencies = data[2].split("_")
            if "task" in task_id_dependencies[0]:
                task_id = f"{independent_task_id}"
                task_dependencies = []
                independent_task_id += 1
            else:
                task_id = task_id_dependencies[0][:1] # Ignore the first letter.
                task_dependencies = task_id_dependencies[1:]
            task_duration = data[3]
            task_cpu = data[4]
            task_mem = data[5]
            task_no_instances = data[6]
            job_dict = jobs_dictionary[job_id]
            if job_dict is None or task_id not in job_dict:
                jobs_dictionary[job_id] = {}
            jobs_dictionary[job_id][task_id] = [task_id, task_dependencies, task_duration, task_cpu, task_mem, task_no_instances]
    return data

def topological_sort(job: dict):
    """
    Sort the tasks in a job in topological order, given that Job is represented as a dictionary with entries of the form
     task: [dependencies].
    Note: CahtGPT generated the following code. I adapted it to my scenario.
    :param job:
    :return: the critial path of the job
    """
    # Create a directed graph
    G = nx.DiGraph()

    # Add nodes (tasks) to the graph
    for task in job:
        if task not in G.nodes:
            G.add_node(task)
        dependencies = job[task][1]
        for dependency in dependencies:
            if dependency not in G.nodes:
                G.add_node(dependency)
            G.add_edge(dependency, task)

    # Calculate the earliest start time for each task
    earliest_start_time = nx.topological_sort(G)
    start_times = {node: 0 for node in G.nodes}

    for node in earliest_start_time:
        for successor in G.successors(node):
            start_times[successor] = max(start_times[successor], start_times[node] + 1)

    # Calculate the latest start time for each task
    latest_start_time = nx.topological_sort(G.reverse())
    end_times = {node: start_times[node] for node in G.nodes}

    for node in latest_start_time:
        for predecessor in G.predecessors(node):
            end_times[predecessor] = min(end_times[predecessor], end_times[node] - 1)

    # Find the critical path
    critical_path = []
    for node in G.nodes:
        if start_times[node] == end_times[node]:
            critical_path.append(node)

    return critical_path


def process_data(filename: str = "alibaba_trace.csv"):
    """
    Process the data by reading the original csv file and extracting the required information for each job, then compiling
    it into the format accepted by PeerSim. And lastly writing the output to a new file. Which can be passed in the configs
    
    We need to compile 3 attributed for each job:

    
    :return: 
    """
    data = read_data_form_csv(filename)
    output_data = {}
    for job in data:
        job_dict = data[job]
        critical_path = topological_sort(job_dict)
        cpu_resources = 0
        mem_resources = 0
        instances = 0
        for task in job_dict:
            cpu_resources += job_dict[task][3]
            mem_resources += job_dict[task][4]
            instances += job_dict[task][5]
        # Write the job to the new file
        output_data[job] = {}
        output_data[job]["critical_path"] = critical_path
        output_data[job]["tasks"] = job_dict

        output_data[job]["resources_cpu"] = cpu_resources
        output_data[job]["resources_mem"] = mem_resources
        output_data[job]["resources_instances"] = instances
    return output_data


if "__name__" == "__main__":
    print("Cleaning up the alibaba trace generated by the All-less/trace-generator")
    output_data = process_data("./Datasets/batch_task.csv")
    with open("./Datasets/alibaba_trace_cleaned.json", "w") as file:
        json.dump(output_data, file)
    print("Done.")

